{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "LBF.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/afrina23/Single-Image-Crowd-Counting-with-MCNN/blob/master/LBF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z42d-2jjb_Q9",
        "colab_type": "code",
        "outputId": "8e842590-9bd7-4673-a809-a3f4434abc44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/gdrive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hN0R8AhcXUUE",
        "colab_type": "code",
        "outputId": "3b87598b-fb0c-4024-fabb-e4f6f279e582",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd gdrive/My Drive/project_folder/LBF_RR"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/project_folder/LBF_RR\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rzz32gC6XnUg",
        "colab_type": "code",
        "outputId": "3be4d4c6-1fe0-4509-d4de-2af37fdf1ea6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "!ls"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "README.md  src\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "268JUCvTXy6P",
        "colab_type": "code",
        "outputId": "1c0b92c6-85e5-4860-8b24-6add6a88bdc2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        }
      },
      "source": [
        "!git clone https://github.com/appstud/Crowd-Counting-With-LBP.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'Crowd-Counting-With-LBP'...\n",
            "remote: Enumerating objects: 33, done.\u001b[K\n",
            "remote: Counting objects: 100% (33/33), done.\u001b[K\n",
            "remote: Compressing objects: 100% (20/20), done.\u001b[K\n",
            "remote: Total 33 (delta 9), reused 33 (delta 9), pack-reused 0\u001b[K\n",
            "Unpacking objects: 100% (33/33), done.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kqsMXtBAX4nN",
        "colab_type": "code",
        "outputId": "1556f81d-3b40-4bbe-e414-9d81240a0637",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%cd Crowd-Counting-With-LBP"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/gdrive/My Drive/project_folder/LBF_RR/Crowd-Counting-With-LBP\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wR8PZamWicUD",
        "colab_type": "code",
        "outputId": "e9597a88-57f2-4e0d-b7a9-303573c6a5a7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 990
        }
      },
      "source": [
        "import scipy.io as sio\n",
        "import cv2\n",
        "import argparse\n",
        "#import utils as utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pdb\n",
        "from skimage import feature\n",
        "import numpy as np\n",
        "import os\n",
        "import glob\n",
        "import os\n",
        "import random\n",
        "\n",
        "\n",
        "def get_file_id(filepath):\n",
        "    return os.path.splitext(os.path.basename(filepath))[0]\n",
        "\n",
        "\n",
        "def get_data_list(data_root, mode='train'):\n",
        "    \"\"\"\n",
        "    Returns a list of images that are to be used during training, validation and testing.\n",
        "    It looks into various folders depending on the mode and prepares the list.\n",
        "    :param mode: selection of appropriate mode from train, validation and test.\n",
        "    :return: a list of filenames of images and corresponding ground truths after random shuffling.\n",
        "    \"\"\"\n",
        "\n",
        "    if mode == 'train':\n",
        "        imagepath = os.path.join(data_root, 'train_data', 'images')\n",
        "        gtpath = os.path.join(data_root, 'train_data', 'ground_truth')\n",
        "\n",
        "    elif mode == 'valid':\n",
        "        imagepath = os.path.join(data_root, 'valid_data', 'images')\n",
        "        gtpath = os.path.join(data_root, 'valid_data', 'ground_truth')\n",
        "\n",
        "    else:\n",
        "        imagepath = os.path.join(data_root, 'test_data', 'images')\n",
        "        gtpath = os.path.join(data_root, 'test_data', 'ground_truth')\n",
        "\n",
        "    image_list = [file for file in glob.glob(os.path.join(imagepath, '*.jpg'))]\n",
        "    gt_list = []\n",
        "\n",
        "    for filepath in image_list:\n",
        "        file_id = get_file_id(filepath)\n",
        "        gt_file_path = os.path.join(gtpath, 'GT_' + file_id + '.mat')\n",
        "        gt_list.append(gt_file_path)\n",
        "    xy = list(zip(image_list, gt_list))\n",
        "    random.shuffle(xy)\n",
        "    s_image_list, s_gt_list = zip(*xy)\n",
        "\n",
        "    return s_image_list, s_gt_list\n",
        "\n",
        "\n",
        "class LocalBinaryPatterns:\n",
        "    def __init__(self, numPoints, radius):\n",
        "        # store the number of points and radius\n",
        "        self.numPoints = numPoints\n",
        "        self.radius = radius\n",
        "\n",
        "    def describe(self, image, eps=1e-7):\n",
        "        # compute the Local Binary Pattern representation\n",
        "        # of the image, and then use the LBP representation\n",
        "        # to build the histogram of patterns\n",
        "        lbp = feature.local_binary_pattern(image, self.numPoints,\n",
        "                self.radius, method=\"uniform\")\n",
        "        (hist, _) = np.histogram(lbp.ravel(),\n",
        "                bins=np.arange(0, 60))\n",
        "\n",
        "        # normalize the histogram\n",
        "        hist = hist.astype(\"float\")\n",
        "        hist /= (hist.sum() + eps)\n",
        "\n",
        "        # return the histogram of Local Binary Patterns\n",
        "        return hist\n",
        "\n",
        "    \n",
        "def getNumberOfPointsInsideRectangle(points,rectangle={\"anchor\":(0,0),\"width\":200,\"height\":200}):\n",
        "    ### points: a 2D array of points, each line is a sample.\n",
        "    ### rectangle: a dictionary that defines a rectangle by its top left corner, width and height.\n",
        "    ### returns the points inside the rectangle and their number.\n",
        "    \n",
        "    mask=(points[:,0]<(rectangle[\"anchor\"][1]+rectangle[\"width\"])) & (points[:,0]> rectangle[\"anchor\"][1])\n",
        "    mask=(mask) & (points[:,1]<(rectangle[\"anchor\"][0]+rectangle[\"height\"])) & (points[:,1]> rectangle[\"anchor\"][0])\n",
        "    \n",
        "    return np.sum(mask),points[mask,:]\n",
        "\n",
        "def drawPointsOnImage(image,points):\n",
        "    ###image: RGB image to draw points on\n",
        "    ###points: the points to be drawn\n",
        "    ###returns: the image after the drawing operation\n",
        "    \n",
        "    for x,y in list(points):\n",
        "        cv2.circle(image, (np.int(x),np.int(y)), 1, (0, 255, 0), -1)\n",
        "    return image\n",
        "\n",
        "def extractDataForOneImage(image,points,lbp,numberOfRectanglePerRow,numberOfRectanglePerColumn,showImg=0):\n",
        "    \n",
        "    \n",
        "    ###Convert to grayscale if the image is not in grayscale already\n",
        "    height,width,numChannels=image.shape\n",
        "    if(numChannels==3):\n",
        "        grayImage=cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "    else:\n",
        "        grayImage=rawImage\n",
        "    nbPointInImage=[]\n",
        "    widthOfPatch=int(width/float(numberOfRectanglePerRow))\n",
        "    heightOfPatch=int(height/float(numberOfRectanglePerColumn))\n",
        "    allHistInImage=[]\n",
        "    \n",
        "    for i in range(numberOfRectanglePerColumn):\n",
        "        for j in range(numberOfRectanglePerRow):\n",
        "            ###Get the points inside a patch and their numbers\n",
        "            nbPoint,headPointsInPatch=getNumberOfPointsInsideRectangle(points,rectangle={\"anchor\":(i*heightOfPatch,j*widthOfPatch),\"width\":widthOfPatch,\"height\":heightOfPatch})\n",
        "            \n",
        "            ###Add it to a vector that contains the number of points in a each patch\n",
        "            nbPointInImage.append(nbPoint)\n",
        "\n",
        "            ###Draw the boundaries of the current patch on the image\n",
        "            cv2.rectangle(image,(j*widthOfPatch,i*heightOfPatch),(j*widthOfPatch+widthOfPatch,i*heightOfPatch+heightOfPatch),(0,0,255),3)\n",
        "            \n",
        "            ###Draw the points available in the current patch on the image\n",
        "            rawImage=drawPointsOnImage(image,headPointsInPatch)\n",
        "\n",
        "            ###Show the image\n",
        "            if (showImg):\n",
        "                cv2.imshow(\"image\",rawImage)\n",
        "                k=cv2.waitKey(1)\n",
        "\n",
        "            ###Calculate the LBP uniform histogram descriptor for the current patch\n",
        "            hist=lbp.describe(grayImage[i*heightOfPatch:(i+1)*heightOfPatch,j*widthOfPatch:(j+1)*widthOfPatch], eps=1e-7)\n",
        "\n",
        "            ###Append current 59D descriptor to descriptors of previous patches in a list\n",
        "            allHistInImage.append(hist)\n",
        "            \n",
        "    ###Concatenate all descriptors of all patches into one descriptor vector  \n",
        "    allHistInImage=np.concatenate(allHistInImage, axis=0)\n",
        "\n",
        "    return nbPointInImage,allHistInImage\n",
        "\n",
        "\n",
        "data_root='/content/gdrive/My Drive/project_folder/LBF_RR/data/ShanghaiTech/part_A_final'\n",
        "numberOfRectanglePerRow=16\n",
        "numberOfRectanglePerColumn=12\n",
        "showImg=0\n",
        "\n",
        "def extractDescriptorsAndGroundTruthMatrix(mode=\"train\"):\n",
        "    #Get the list of images.\n",
        "    images_list, gts_list = get_data_list(data_root, mode=mode)\n",
        "    allPoints=None\n",
        "    allHist=None\n",
        "    lbp=LocalBinaryPatterns(numPoints=8, radius=1)\n",
        "    for img_idx in range(len(images_list)):\n",
        "        \n",
        "        # Load the image and ground truth\n",
        "        rawImage = np.asarray(cv2.imread(images_list[img_idx]), dtype=np.uint8)\n",
        "        matFile=sio.loadmat(gts_list[img_idx])\n",
        "        if img_idx%10 == 0:\n",
        "            print(\"Image processed : \"+str(img_idx)+\"/\"+str(len(images_list)))\n",
        "        headPoints = matFile['image_info'][0][0][0][0][0]\n",
        "            \n",
        "        nbPointInImage,allHistInImage=extractDataForOneImage(rawImage,headPoints,lbp,numberOfRectanglePerRow,numberOfRectanglePerColumn,showImg)\n",
        "        \n",
        "        ###Concatenate all descriptors into one matrix and all vectors of number of points into one matrix   \n",
        "        if(allPoints is None):\n",
        "            allPoints=np.array(nbPointInImage)\n",
        "        else:\n",
        "            allPoints=np.vstack((allPoints,nbPointInImage))\n",
        "\n",
        "        if(allHist is None):\n",
        "            allHist=allHistInImage\n",
        "\n",
        "        else:\n",
        "            allHist=np.vstack((allHist,allHistInImage))\n",
        "        \n",
        "    ###Save the extracted descriptors matrix and ground truth matrix\n",
        "    sio.savemat(os.path.join(data_root,mode+\".mat\"),{\"lbp_descriptors\":allHist,\"labels\":allPoints})\n",
        "      \n",
        "    print(mode+ \"set descriptors shape:\" +str(allHist.shape))\n",
        "    print(mode+ \"set ground truth shape:\" +str(allPoints.shape))\n",
        "\n",
        "def main():\n",
        "    ###Extract descriptors and ground truth for the train set\n",
        "    extractDescriptorsAndGroundTruthMatrix(mode=\"train\")\n",
        "\n",
        "    ###Extract descriptors and ground truth for the test set\n",
        "    extractDescriptorsAndGroundTruthMatrix(mode=\"test\")\n",
        "    \n",
        "\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    #parser = argparse.ArgumentParser()\n",
        "    #parser.add_argument('--data_root', default='/content/gdrive/My Drive/project_folder/LBF_RR/data/ShanghaiTech/part_A_final', type=str)\n",
        "    #parser.add_argument('--numberOfRectanglePerRow', default=16, type=int)\n",
        "    #parser.add_argument('--numberOfRectanglePerColumn', default=12, type=int)\n",
        "    #parser.add_argument('--showImg', default=0, type=int)\n",
        "\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    main()\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Image processed : 0/300\n",
            "Image processed : 10/300\n",
            "Image processed : 20/300\n",
            "Image processed : 30/300\n",
            "Image processed : 40/300\n",
            "Image processed : 50/300\n",
            "Image processed : 60/300\n",
            "Image processed : 70/300\n",
            "Image processed : 80/300\n",
            "Image processed : 90/300\n",
            "Image processed : 100/300\n",
            "Image processed : 110/300\n",
            "Image processed : 120/300\n",
            "Image processed : 130/300\n",
            "Image processed : 140/300\n",
            "Image processed : 150/300\n",
            "Image processed : 160/300\n",
            "Image processed : 170/300\n",
            "Image processed : 180/300\n",
            "Image processed : 190/300\n",
            "Image processed : 200/300\n",
            "Image processed : 210/300\n",
            "Image processed : 220/300\n",
            "Image processed : 230/300\n",
            "Image processed : 240/300\n",
            "Image processed : 250/300\n",
            "Image processed : 260/300\n",
            "Image processed : 270/300\n",
            "Image processed : 280/300\n",
            "Image processed : 290/300\n",
            "trainset descriptors shape:(300, 11328)\n",
            "trainset ground truth shape:(300, 192)\n",
            "Image processed : 0/182\n",
            "Image processed : 10/182\n",
            "Image processed : 20/182\n",
            "Image processed : 30/182\n",
            "Image processed : 40/182\n",
            "Image processed : 50/182\n",
            "Image processed : 60/182\n",
            "Image processed : 70/182\n",
            "Image processed : 80/182\n",
            "Image processed : 90/182\n",
            "Image processed : 100/182\n",
            "Image processed : 110/182\n",
            "Image processed : 120/182\n",
            "Image processed : 130/182\n",
            "Image processed : 140/182\n",
            "Image processed : 150/182\n",
            "Image processed : 160/182\n",
            "Image processed : 170/182\n",
            "Image processed : 180/182\n",
            "testset descriptors shape:(182, 11328)\n",
            "testset ground truth shape:(182, 192)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FUWZEeKpjO41",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%tb"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7toy3ulvnvGo",
        "colab_type": "text"
      },
      "source": [
        "train the model below"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D3RftvNxnw3F",
        "colab_type": "code",
        "outputId": "4a04a6da-c795-42bc-a307-cf5f736dfe81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 166
        }
      },
      "source": [
        "import scipy.io as sio\n",
        "import cv2\n",
        "import argparse\n",
        "#import utils as utils\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "import pdb\n",
        "from skimage import feature\n",
        "import numpy as np\n",
        "import os\n",
        "import time \n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "from sklearn.metrics import make_scorer\n",
        "\n",
        "def computeError(y, y_pred):\n",
        "    return np.sqrt(np.mean((np.sum(y_pred,1)-np.sum(y,1))**2))\n",
        "def computeMAE(y, y_pred):\n",
        "    return np.mean((np.sum(y_pred,1)-np.sum(y,1)))\n",
        "\n",
        "    \n",
        "data_root='/content/gdrive/My Drive/project_folder/LBF_RR/data/ShanghaiTech/part_A_final'\n",
        "def main():\n",
        "    \n",
        "\n",
        "    ###Read descriptors and ground truth for train set\n",
        "    matFile=sio.loadmat(os.path.join(data_root,\"train\"))\n",
        "    descriptorsTrain=matFile[\"lbp_descriptors\"]\n",
        "    labelsTrain=matFile[\"labels\"]\n",
        "\n",
        "    ###Read descriptors and ground truth for test set\n",
        "    matFile2=sio.loadmat(os.path.join(data_root,\"test\"))\n",
        "    descriptorsTest=matFile2[\"lbp_descriptors\"]\n",
        "    labelsTest=matFile2[\"labels\"]\n",
        "    \n",
        "    print(\"descriptors train shape:\"+ str(descriptorsTrain.shape))\n",
        "    print(\"labels train shape:\"+ str(labelsTrain.shape)+\"\\n\")\n",
        "    print(\"descriptors test shape:\"+ str(descriptorsTest.shape))\n",
        "    print(\"labels test shape:\"+ str(labelsTest.shape)+\"\\n\")\n",
        "    \n",
        "    Grid_Dict = {\"alpha\": [1e-13,1e-5,1e-4,1e-3,1e-2],\"gamma\": np.logspace(-3, 2, 10)}\n",
        "    krr_Tuned = GridSearchCV(KernelRidge(kernel='rbf'), cv=4 ,param_grid=Grid_Dict, scoring=make_scorer(computeError,greater_is_better=False),refit=True)\n",
        "    krr_Tuned.fit(descriptorsTrain, labelsTrain)\n",
        "\n",
        "    \n",
        "    predictionsTrain=krr_Tuned.predict(descriptorsTrain)\n",
        "    MSE_train=computeError(labelsTrain, predictionsTrain)\n",
        "    MAE_train=computeMAE(labelsTrain, predictionsTrain)\n",
        "\n",
        "\n",
        "    ########testing###########  \n",
        "    predictionsTest=krr_Tuned.predict(descriptorsTest)\n",
        "    MSE_test=computeError(labelsTest, predictionsTest)\n",
        "    MAE_test=computeMAE(labelsTest, predictionsTest)\n",
        "    \n",
        "\n",
        "    print(\"  minTestMSE:\"+str( MSE_test),\"  minTestMAE:\"+str( MAE_test),  \" minTrainMSE:\"+ str(MSE_train),\" minTrainMAE:\"+ str(MAE_train),\n",
        "          \"Best alpha:\"+ str(krr_Tuned.best_params_['alpha']), \"Best rbf param:\"+ str(krr_Tuned.best_params_['gamma']))\n",
        "\n",
        "    \n",
        "        \n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \n",
        "    #parser = argparse.ArgumentParser()\n",
        "    #parser.add_argument('--data_root', default='/content/gdrive/My Drive/project_folder/LBF_RR/data/ShanghaiTech/part_A_final', type=str)\n",
        "    #args = parser.parse_args()\n",
        "\n",
        "    main()\n",
        "    \n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "descriptors train shape:(300, 11328)\n",
            "labels train shape:(300, 192)\n",
            "\n",
            "descriptors test shape:(182, 11328)\n",
            "labels test shape:(182, 192)\n",
            "\n",
            "  minTestMSE:434.6370654022479   minTestMAE:111.07478957206551  minTrainMSE:99.17123841283417  minTrainMAE:0.0237269178540555 Best alpha:0.01 Best rbf param:0.046415888336127795\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}